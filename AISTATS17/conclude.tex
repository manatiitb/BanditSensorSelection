
We introduced an online learning setup named unsupervised sequential sensor acquisition (USS) where sensors are arranged in a cascade and can be acquired sequentially on incremental payment. The novelty of the setup is predictions of the sensors do not reveal information about the loss incurred, but the goal is still to identify an optimal sensor. We introduced the notion of strong and weak dominance and showed that optimal sensor can be identified when strong dominance (SD) property holds and weak dominace (WD) is the maximal extension of the SD property. We verified that many real datasets satisfy the WD property and developed algorithms that work effectively. We briefly discuss few possible extensions of our setup below.

\noindent
{\bf Extensions:}

\noindent
{\it Tree-Architectures:}
%It is straightforward to see that we can generalize our cascade model to a tree-architecture. 
%The algorithm Alg.~\ref{alg:asym} and Alg.~\ref{alg:UCB} extends to trees. 
We can deal with trees in an analogous fashion. Like for cascades we keep track of disagreements along each path. Under SD these disagreements in turn approximate marginal error. This fact is sufficient to identify the optimal sensor (see Eqn.~\ref{eqn:interp_opt}). Similarly our results also generalize to trees under WD condition. 

\noindent
{\it Context-Dependent Sensor Selection:}
%Our results can also be generalized to specific context dependent cases as well. 
Each example has a context $x \in \mathbb{R}^d$. A test is a mapping that takes (partial) context and maps to an output space. We briefly describe how to extend to context-dependent sensor selection. Analogous to the context-independent case we impose context dependent notions for SD and WD, namely, Eq.~\ref{eqn:DominanceCondition} and Eq.~\ref{eqn:WD} hold conditioned on each $x \in {\cal X}$. To handle these cases we let $\gamma_i(x)\triangleq \Pr(Y^{i} \neq Y)$ and $\gamma_{ij}(x) \triangleq \Pr(Y^{i} \neq Y^{j})$ denote the conditional error probability for the ith sensor and the conditional marginal error probability between sensor $i$ and $j$ respectively. Then the results can be generalized under a parameterized GLM model for disagreement, namely, the log-odds ratio $\log \frac{\gamma_{ij}(x)}{1-\gamma_{ij}(x)}$ can be written as $\theta_{ij}'x$ for some unknown $\theta_{ij} \in \mathbb{R}^d$.
%\begin{proof}