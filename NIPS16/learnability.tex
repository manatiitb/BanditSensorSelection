%!TEX root =  main.tex
\newcommand{\SA}{\mathrm{SA}}
\newcommand{\SD}{\mathrm{SD}}
\newcommand{\WD}{\mathrm{WD}}
\newcommand{\TSA}{\Theta_{\SA}}
\newcommand{\Alg}{\mathfrak{A}}
\newcommand{\TSD}{\Theta_{\SD}}
\newcommand{\TWD}{\Theta_{\WD}}
Let $\TSA$ be the set of all sensor acquisition problems. \todoc{Shall we define ``proper'' problems?}
Thus, $\theta = (P,c)\in \TSA$ such that if $Y\sim P$ then $\gamma_k(\theta):=\Prob{Y\ne Y^k}$ 
is a decreasing sequence.
Given a subset $\Theta\subset \TSA$, we say that $\Theta$ is \emph{learnable} 
if there exists a learning algorithm $\Alg$ such that
for any $\theta\in \Theta$, the expected regret $\EE{ \Regret_n(\Alg,\theta) }$ 
of algorithm $\Alg$ on instance $\theta$ is sublinear.
A subset $\Theta$ is said to be a maximal learnable problem class if it is learnable and for any $\Theta'\subset \TSA$ superset
of $\Theta$, $\Theta'$ is not learnable.
In this section we study two special learnable problem classes, $\TSD\subset \TWD$, where the instances in $\TSD$ are easier to identify in practice, while $\TWD$ can be seen as a maximal extension of $\TSD$.

Let us start with some definitions.
Given an instance $\theta = (P,c)\in \TSA$, we can decompose $P$ into the joint distribution $P_S$ of the sensor outputs $S = (Y^1,\dots,Y^k)$ and the conditional distribution of the state of the environment, given the sensor outputs, $P_{Y|S}$.
Specifically, letting $(Y,S)\sim P$, for $s\in \{0,1\}^K$ and $y\in \{0,1\}$, $P_S(s) = \Prob{S = s}$ and $P_{Y|S}(y|s) = \Prob{Y=y|S=s}$. We denote this by $P = P_S \otimes P_{Y|S}$.
A learner who observes the output of all sensors for long enough is able to identify $P_S$ with arbitrary precision, while $P_{Y|S}$ remains hidden from the learner. This leads to the following simple statement whose proof is left as an exercise:
\begin{proposition}
\label{prop:learnablemap}
$\Theta\subset \TSA$ is learnable if and only if there exists a map $a: M_1( \{0,1\}^K ) \to [K]$ such that 
for any $\theta= (P,c)$, if $P = P_S \otimes P_{Y|S}$ then $a(P_S)$ is an optimal action in $\theta$.
\end{proposition}

A class of sensor acquisition problems that contains instances that satisfy the so-called \emph{strong dominance} condition 
will be shown to be learnable:
\begin{definition}[Strong Dominance]
	An instance $\theta = (P,c)\in \TSA$  is said to satisfy the \emph{strong dominance property} if 
	it holds in the instance that if a sensor predicts correctly
	then all the sensors in the subsequent stages of the cascade also predict correctly, i.e., 
	for any $i\in [K]$,
	\begin{equation}
	\label{eqn:DominanceCondition}
	Y^i=Y \,\, \Rightarrow\,\, Y^{i+1}= \dots =  Y^K = Y
	\end{equation}
	almost surely (a.s.)
	where $(Y,Y^1,\dots,Y^K)\sim P$.
\end{definition}
Let $\TWD = \{ \theta\in \TSA\,:\, \theta \text{ satisfies the strong dominance condition } \}$.
\begin{thm}
The set $\TWD$ is learnable.
\end{thm}
\begin{proof}
We construct a map as required by~\cref{prop:learnablemap}.
Take an instance $\theta = (P,c)\in \TWD$ and let $P = P_S \otimes P_{Y|S}$ be its decomposition
as defined above.
Let $\gamma_i = \Prob{Y^i \ne Y}$, $(Y,Y^1,\dots,Y^K)\sim P$, $C_i = c_1+\dots+c_i$.
For identifying an optimal action in $\theta$, it clearly suffices
to know the sign of $\gamma_i + C_i - (\gamma_j +C_j)$ for all pairs $i,j\in [K]^2$.
Since $C_i - C_j$ is known, it remains to study $\gamma_i-\gamma_j$.
Without loss of generality (WLOG) let $i<j$.
Then, 
\begin{align*}
0 & \le \gamma_i  - \gamma_j = \Prob{Y^i\ne Y} - \Prob{Y^j\ne Y} \\
& = \cancel{\Prob{Y^i\ne Y, Y^i=Y^j}} + \Prob{ Y^i\ne Y, Y^i\ne Y^j } - \\
& - \left\{ 
       \cancel{\Prob{Y^j\ne Y, Y^i=Y^j}} + \Prob{ Y^j\ne Y, Y^i\ne Y^j }\right\}\\
& = \Prob{ Y^i\ne Y, Y^i \ne Y^j } + \Prob{Y^i=Y,Y^i\ne Y^j}       \\
& - \left\{ 
	  \Prob{ Y^j \ne Y, Y^i\ne Y^j } + \Prob{ Y^i=Y,Y^i\ne Y^j}
	 \right\}\\
& \stackrel{\footnotesize (a)}{=} \Prob{ Y^j \ne Y^i } -2 \Prob{ Y^j\ne Y, Y^i = Y } \,,
\numberthis
\label{eq:keyidentity}
\end{align*}
where in $(a)$ we used that $\Prob{ Y^j \ne Y, Y^i\ne Y^j } =  \Prob{ Y^j\ne Y,Y^i= Y}$ and also
$\Prob{ Y^i=Y,Y^i\ne Y^j} = \Prob{ Y^j\ne Y,Y^i= Y}$ because $Y,Y^i,Y^j$ only take on two possible values.
Now, since $\theta$ satisfies the strong dominance condition, $ \Prob{ Y^j\ne Y, Y^i = Y } = 0$.
Thus,
\begin{align*}
\gamma_i - \gamma_j = \Prob{ Y^j \ne Y^i }\,,
\end{align*}
which is a function of $P_S$ only.
Thus, a map as required by~\cref{prop:learnablemap} exists.
\end{proof}
The proof motivates the following definition:
\begin{definition}[Weak Dominance]
	An instance $\theta = (P,c)\in \TSA$  is said to satisfy the \emph{weak dominance property} if 
	it holds that for any $i<j\in [K]$,
\begin{align}\label{eq:wd}
	C_i-C_j \notin [ \Prob{ Y^j \ne Y^i } -2 \Prob{ Y^j\ne Y, Y^i = Y }, \Prob{ Y^j \ne Y^i } ]\,,
\end{align}
where $(Y,Y^1,\dots,Y^K)\sim P$ and $C_i = c_1+\dots+c_i$, $i\in [K]$.
\end{definition}
\todoc[inline]{Note that it is NOT enough to consider $j=i+1$ (i.e., immediate successors) in the definition.
This is because one may need to compare all pairs to find the optimal decision -- comparing successors is not enough.
To see why, consider an instance when $\gamma_1,\gamma_2,\gamma_3$ are given by $0.2,0.2,0$, while
$(C_1,C_2,C_3 )= (0,0.1,0.1)$.
Then, the costs of the actions are $(0.2,0.3,0.1)$. In particular, the first index where we see the total cost grow does not give
the optimal action. Similarly, the last index would not work either.
}
Let $\TWD = \{ \theta\in \TSA\,:\, \theta \text{ satisfies the weak dominance condition } \}$.
Note that $\TSD\subset \TWD$.
\begin{thm}
The set $\TWD$ is a maximal learnable set.
\end{thm}
\begin{proof}
That $\TWD$ is learnable follows from \eqref{eq:keyidentity} combined with~\eqref{eq:wd}.
In particular, fixing $i<j\in [K]$, if $C_j-C_i<\Prob{ Y^j \ne Y^i } -2 \Prob{ Y^j\ne Y, Y^i = Y }$ then
\begin{align*}
\gamma_i+C_i-(\gamma_j+C_j) 
% =    (\gamma_i-\gamma_j) - (C_j-C_i) 
 = \Prob{ Y^j \ne Y^i } -2 \Prob{ Y^j\ne Y, Y^i = Y } - (C_j-C_i)  >  0\,.
\end{align*}
On the other hand, if $C_j-C_i>\Prob{Y^j\ne Y^i}$,
\begin{align*}
\gamma_i+C_i-(\gamma_j+C_j) 
% =    (\gamma_i-\gamma_j) - (C_j-C_i) 
& = \Prob{ Y^j \ne Y^i } -2 \Prob{ Y^j\ne Y, Y^i = Y } - (C_j-C_i) \\
 & <  -2 \Prob{ Y^j\ne Y, Y^i = Y } \le 0 \,.
\end{align*}
In particular, for an instance satisfying the weak dominance condition we see that
$\gamma_i+C_i-(\gamma_j+C_j) <0$ holds if and only if $C_j-C_i>\Prob{Y^j\ne Y^i}$, a condition
that can be verified given the knowledge of $P_S$ alone (where recall that $P = P_S \otimes P_{Y|S}$).
Thus, a map as required by \cref{prop:learnablemap} exists. Note also that this map is uniquely defined for all instances
where the optimal action is unique.
That $\TWD$ is maximal follows because if we add an instance $\theta\in \TSA \setminus \TWD$ 
\todoc[inline]{There is a problem with the definition of maximality. A set can always be ``lucky''..
Non-unique optimal actions also cause problems.}
\end{proof}

\todoc[inline]{Old text}
In the SA-Problem feedback $H_t(\cdot)$ does not reveal any information about the true label $Y_t$ in any round $t$. Hence the loss values are not known, and we are in a hopeless situation where linear regret is unavoidable. In this section we explore conditions that lead to policies that are Hannan consistent \cite{Hannan1957_HannanConsistency_Hannan}, i.e, a policy $\pi\in \Pi^\psi$ such that $R_T^\psi (\pi)/T \rightarrow 0$.

To fix ideas let us consider SA-Problem with $2$ sensors. We enumerate all possible $8$ tuples $(Y, \hat{Y}^1, \hat{Y}^2)$ as shown in Table \ref{tab:SensorOutcomes}, and write probability of $i$th tuple $i=1,2,\cdots 8$ as $p_{i-1}$.  From Table \ref{tab:SensorOutcomes}, we have  $\gamma_1=p_2+p_3+p_4+p_5$ and $\gamma_2=p_1+p_3+p_4+p_6$, thus
\begin{equation}
	\gamma_1-\gamma_2 = p_2+p_5-p_1-p_6.
\end{equation}

\begin{minipage}{0.5\textwidth}
	\vspace{.5cm}
	\begin{tabular}[c]{ c|c|c|c } 
		\label{tab:SensorOutcomes}
	%			\caption{Possible binary tuples}
		$Y$ & $\hat{Y}^1$ & $\hat{Y}^2$ & $\Pr(Y, \hat{Y}^1, \hat{Y}^2)$ \\ \hline 
		$0$ & $0$ & $0$ & $p_0$ \\  \hline
		$0$ & $0$ & $1$ & $p_1$ \\  \hline
		$0$ & $1$ & $0$ & $p_2$ \\  \hline
		$0$ & $1$ & $1$ & $p_3$ \\  \hline
		$1$ & $0$ & $0$ & $p_4$ \\  \hline
		$1$ & $0$ & $1$ & $p_5$ \\  \hline
		$1$ & $1$ & $0$ & $p_6$ \\  \hline
		$1$ & $1$ & $1$ & $p_7$ \\  \hline
		
	\end{tabular}
		\vspace{.5cm}
\end{minipage}\hspace{-1.5cm}
\begin{minipage}[c]{0.6\textwidth}
		\vspace{.5cm}
		\centering
	\hspace{-5cm}		
\begin{equation}
\label{eqn:Marginals}
\Pr(\hat{Y}^1, \hat{Y}^2)=
\begin{cases}
p_1 + p_5 \mbox{  if } (\hat{Y}^1, \hat{Y}^2)=(0,1)\\
p_2 + p_6 \mbox{  if } (\hat{Y}^1, \hat{Y}^2)=(1,0)\\
p_0 + p_4 \mbox{  if } (\hat{Y}^1, \hat{Y}^2)=(0,0)\\
p_3 + p_7 \mbox{  if } (\hat{Y}^1, \hat{Y}^2)=(1,1)
\end{cases}
\end{equation}
	\vspace{.5cm}
\end{minipage}

\noindent
Since we only observe feedbacks $(\hat{Y}_t^1, \hat{Y}_t^2)$ and not the true labels $Y_t$, only marginal probabilities $\Pr(\hat{Y}^1, \hat{Y}^2)$ as given in (\ref{eqn:Marginals}) can be estimated but not $\Pr(Y, \hat{Y}^1, \hat{Y}^2)$. Thus all the decision has to be based on the marginals only. To see when SAP has a Hannan consistent policy, let us consider the following conditions.
\begin{condition}
	\label{cond:PathDominance1}
	 If sensor $1$ predicts label $1$ correctly, then sensor $2$ also predicts it correctly\footnote{Suppose we interpret label $1$ as 'threat', the condition implies that if sensor $1$ detects threat correctly, the better sensor $2$ also detects it. }, i.e.,
	 \begin{equation*}
	 \label{eqn:PathDominace1} 
	 Y_t=1 \mbox{ and } \hat{Y}_t^1=1 \implies \hat{Y}^2_t=1. 
	 \end{equation*}
\end{condition} 
\begin{condition}
		\label{cond:PathDominance2}
	If sensor $1$ predicts label $0$ correctly, then sensor $2$ also predicts it correctly, i.e.,
	\begin{equation*}
	\label{eqn:PathDominace2} 
	Y_t=0 \mbox{ and } \hat{Y}_t^1=0 \implies \hat{Y}^2_t=0. 
	\end{equation*}
\end{condition}
The following example demonstrate  marginals do not unambiguously decide optimal action under Condition \ref{cond:PathDominance1}.
Set $c=0.35$ and consider the following two cases: 1) $p_2=1/2, p_1=1/4-1/40, p_5=1/4+1/40$ and 2) $p_2=1/2, p_1=1/4-3/40,p_5=1/4+3/40$. From Condition (\ref{cond:PathDominance1}) we have $p_6=0$. Also, set $p_0=p_4=p_3=p_7=0$ in both the cases. We get $\gamma_1-\gamma_2=0.3$ in the first case,  whereas $\gamma_1-\gamma_2=0.4$ in the second case. From \ref{eqn:OptimalAction}, optimal action is $1$ in the first case, whereas it is  $2$ in the second case. However, for both the cases the marginals $\Pr(\hat{Y}^1, \hat{Y}^2)$ are the same for all pairs $(\hat{Y}^1, \hat{Y}^2)$. Since we only  observe $\Pr(\hat{Y}^1, \hat{Y}^2)$, the two cases cannot be distinguished and linear regret is unavoidable. We can argue similarly that Condition (\ref{cond:PathDominance2}) is not sufficient for sub-linear regret. 

Next, consider that both Condition (\ref{cond:PathDominance1}) and (\ref{cond:PathDominance2}) hold, i.e.,  
\begin{condition}
	\label{cond:PathDominance}
	If sensor $1$ is correct , then sensor $2$ is also correct, i.e.,
	\begin{equation*}
	\label{eqn:Dominace} 
	 \hat{Y}_t^1=Y_t \implies \hat{Y}^2_t=Y_t. 
	\end{equation*}
\end{condition}
Then, $p_1=p_6=0$ and we get $\gamma_1-\gamma_2=p_2+p_5$. Since $p_2+p_5=\Pr(\hat{Y}^1 \neq \hat{Y}^2)$, it can be estimated from observations $(\hat{Y}_t^1,\hat{Y}_t^2)$, and the optimal action can be found unambiguously. Thus Condition \ref{cond:PathDominance} gives a sufficient for existence of an Hannan consistent policy. In the following we refer to Condition (\ref{cond:PathDominance}) as strong dominance property. For the case of $K>2$ sensors, its definition is as follows: 

\begin{definition}[Strong Dominance]
	A SA-Problem is said to satisfy strong dominance property if sensor $i$ predicts correctly, then all the sensors in the subsequent stages of the cascade also predict correctly, i.e.,
	\begin{equation}
	\label{eqn:DominanceCondition}
	\hat{Y}_t^i=Y_t \rightarrow \hat{Y}_t^j \quad \forall j>i\geq 1.
	\end{equation}
\end{definition}

We will now establish necessary and sufficient conditions for SAP learnability
For notional convenience rewrite 
$\gamma_1- \gamma_2= p_1+p_2+p_5+p_6- 2(p_1+p_6):=p_{12}-2\delta$,
where $p_{12}:=\Pr(Y^1\neq Y^2)$ is the probability that sensors disagree and $\delta:=\Pr(Y^2 \neq Y | Y^1=Y)$ is the conditional probability that sensor $2$ is incorrect given that sensor $1$ is correct. We can estimate $p_{12}$ from feedback $(\hat{Y}^1_t, \hat{Y}^2_t)$, but $\delta$ cannot be estimated.
\begin{thm}
For SA-Problem with $K=2$, an Hannan consistent policy exists if and only if $c \notin [p_{12}-2\delta, p_{12}]$.
\end{thm}
{\bf Proof:} Under dominance condition $\delta=0$, thus actions $1$ is optimal if $p_{12}<c$, otherwise action $2$ is optimal. Suppose dominance condition is violated, i.e., $\delta>0$, but decisions are made assuming dominance condition holds (i.e., using estimates of $p_{12}$ only), then the optimal action is correctly identified provided $\delta$ is such that $p_{12}-2\delta < c \Rightarrow p_{12} <c$ or $p_{12}-2\delta >c \Rightarrow p_{12}>c$. Now, notice that the latter implication is always true. So, whenever action $2$ is optimal, violation of dominance condition does not miss the optimal action. However, the first implication holds if and only if $c \notin [p_{12}-2\delta, p_{12}]$.

Clearly, when $\delta$ is small Hannan consistent policy exits for a large range of $c$. For the case of $K>2$ sensors, its definition is as follows: 

\begin{definition}[Weak Dominance]
	A SA-Problem is said to satisfy weak dominance property if $c_k \notin [p_{k-1,k}-2\delta_{k-1,k}, p_{k-1,k}]$ for all $1<k<K$, where $p_{k-1,k}=\Pr(Y^{k-1}\neq Y^k)$ and $\delta_{k-1,k}=\Pr(Y^k\neq Y| Y^{k-1}=Y).$
\end{definition}

Many real world applications are designed to satisfy strong dominance property. For example, in wireless communication, increasing block length (more redundancy) improves tolerance against noise. Many practical datasets like, PIMA diabetes dataset and breast cancer dataset, conditional error probabilities are small.   (i will add numerical values)

In the following we establish that if dominance property holds efficient algorithms for a SAP problem can be derived from algorithms on a suitable stochastic multi-armed bandit problem. We first recall the stochastic multi-armed bandit setting and the relevant results. 

%\section{Problem Setup}
%\label{sec:Setup}
%We consider the problem of efficient label prediction under partial monitoring. Let $\{Y_t\}_{{t>0}}$ denote sequence of labels generated according to an unknown distribution $\mathcal{D}$.  The learner can use a `cheap' sensor (device-1) or/and a `costly' sensor (device-2) to predict the labels. In round $t$, let $\hat{Y}^1_t$ and $\hat{Y}^2_t$ denote the predictions of device-1 and device-2 respectively. We assume that device-1 has lower performance than device-2 in the sense that prediction error rate of device-1, denoted as 
%$\gamma_1:=\Pr\{Y_t\neq \hat{Y}^1_t\}$, is larger than or equal to that of device-2, denoted as $\gamma_2:=\Pr\{Y_t\neq \hat{Y}^2_t\}$ ($\gamma_1>\gamma_2$). In each round $t$, the learner can take the following actions: 
%\begin{itemize}
%	\item Action-1: use device-1.
% \item Action-2: use both the devices. 
%\end{itemize}
%For ease of notion, we denote actions by their index, and write $\mathcal{A}=\{1,2\}$ for the set of actions and use $i$ to index them. Let $F_t(i)$ denote the feedback observed in round $t$ by selecting action $i$. When $i=1$, the learner observes $\hat{Y}^1_t$, and when $i=2$, he observes both $\hat{Y}^1_t$ and $\hat{Y}^2_t$. That is, 
%\begin{equation}
%F_t(i)=\begin{cases}
%\hat{Y}_t^1 \;\;\mbox{if}\;\;i=1,\\
%\{\hat{Y}^1_t, \hat{Y}^2_t\} \;\;\mbox{if}\;\;i=2.
%\end{cases}
%\end{equation} 
%The loss incurred in each round is defined as follows. When $i=1$, the loss is $1$ unit if prediction of device-1 (observed feedback) is incorrect, otherwise loss is zero. When $i=2$, a fixed loss of $c>0$ is incurred in addition to the prediction loss of device-2, which is $1$ unit if device-2's prediction is incorrect and $0$ otherwise. Let $L_t(i)$ denote the loss in round $t$ for taking action $i\in \mathcal{A}$. Then,
%\begin{equation}
%L_t(i)=\begin{cases}
%\boldsymbol{1}_{\{\hat{Y}^1_t \neq Y_t\}} \;\;\mbox{if}\;\;i=1,\\
%\boldsymbol{1}_{\{\hat{Y}^2_t\neq Y_t\}}+c \;\;\mbox{if}\;\;i=2.
%\end{cases}
%\end{equation} 
%Let $H_{t}$ denote the set of actions played and the corresponding feedback observed till time $t$.  A policy $\pi=(\pi_1, \pi_2, \cdots)$, where  $\pi_t : H_{t-1}\rightarrow
%\mathcal{A}$, gives action selected in each round using all the feedback observed in the past. The expected regret of a policy $\pi$ that selects action $\pi_t \in\mathcal{A}$ in round $t$ over a period $T$ with respect to the best action in hindsight is given as 
%\begin{equation}
%\label{eqn:Regret2Actions}
%R_T(\pi)= \mathbb{E}\left [\sum_{t=1}^TL_t(\pi_t)\right]
%-\min_{i\in \mathcal{A}}\mathbb{E}\left [\sum_{t=1}^T L_t(i)\right ].
%\end{equation}
%The goal of the learner is to learn a policy that minimizes the maximum expected regret, i.e.,
%\begin{equation}
%\pi^*= \arg \min_{\pi \in \Pi }R_T(\pi)  ,
%\end{equation}
%where $\Pi$ denote the set of policies that maps past history to an action in $\mathcal{A}$ in each round. 
%
%\noindent
%{\bf Optimal action in hindsight: } For any $\pi \in \Pi$ and in any round $t$ we have 
%\begin{equation}
%\mathbb{E}[L_t(i)]=
%\begin{cases}
%\gamma_1\;\; \mbox{if}\;\; i=1,\\
%\gamma_2+c\;\; \mbox{if}\;\;i=2,
%\end{cases}
%\end{equation}
%Let  $i^*=\arg\min_{i \in \mathcal{A}} {E}[L_t(i)]$ denote optimal action. Then, $i^*=1$ if $\gamma_1 \leq \gamma_2 +c$, and $i^*=2$ otherwise. Let $I_t$ denote the action taken in round $t$ and $N_i(s)$ denote the number of times action $i$ is selected till time $s$, i.e., $N_i(s)=\sum_{t=1}^s \boldsymbol{1}_{\{I_t=i\}}$. The expected regret can be expressed as
%\begin{eqnarray}
%\label{eqn:ExpRegretGap}
%R_T(\pi)&=& \sum_{i=1}^{2}\mathbb{E}[N_i(T)]\Delta_i 
%\end{eqnarray}
%where $\Delta_1=\gamma_1 -\mathbb{E}[L(i^*)]$ and $
%\Delta_2=\gamma_2+c -\mathbb{E}[L(i^*)]$. Note that for all $i=1,2$, either $\Delta_i=|\gamma_1-\gamma_2-c|$ or $\Delta_i=0$.
%
%
%\noindent
%{\bf Assumptions (Dominance condition):}   Whenever device-1 makes no prediction error, device-2 is also guaranteed to make no prediction error, i.e., in every round $t$, 
%\begin{equation}
%\label{eqn:DomAssum}
%{\hat{Y}^1_t=Y_t} \implies {\hat{Y}^2_t=Y_t}.
%\end{equation}  
%\noindent
%{\bf Reduction to the apple tasting problem:}
%The feedback from action $i=1$ reveals no information about the loss incurred in that round. However feedback after action $i=2$ reveals (partial) information about the loss of both actions. Suppose feedback is such that the predictions of devices disagree, i.e., ${\hat{Y}^1_t\neq\hat{Y}^2_t}$ after action $2$.  The dominance condition then implies that the only possible events are $\hat{Y}^1_t \neq Y_t$ and $\hat{Y}^2_t=Y_t$. I.e., the true label is that predicted by device-2 and loss is zero. Suppose the predictions of the devices agree, i.e., ${\hat{Y}^1_t = \hat{Y}^2_t}$, then the dominance condition implies that either predictions of both are correct or both are incorrect. Though the true loss is not known in this case, the learner can infer some useful knowledge: in round $t$, if the prediction of both the devices agree, then the difference of loss of the actions is $L_t(2)-L_t(1)=c>0$. And if the predictions of the devices disagree, then dominance assumption implies that $L_t(1)=1$ and $L_t(2)=c$ or $L_t(2)-L_t(1)=c-1<0$. Thus, each time learner selects action $2$, he gets to know whether or not he was better off by selecting the other action. This setup is similar the standard apple tasting problem \cite{IC2000_AppleTasting_HelmboldLittlestoneLong}
%], but differs in terms of the information structure when action $2$ is played: in the apple tasting problem, playing action $2$ in a round reveals loss incurred by both the actions. Whereas, in the sensor selection problem we get only partial information on which of the two actions is better in that round. However, we will see below that the partial information is enough to distinguish the optimal arm and one can obtain performance similar to that in the apple tasting problem .

