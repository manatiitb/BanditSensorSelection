%!TEX root =  main.tex
In the SA-Problem feedback $H_t(\cdot)$ does not reveal any information about the true label $Y_t$ in any round $t$. Hence the loss values are not known, and we are in a hopeless situation where linear regret is unavoidable. In this section we explore conditions that lead to policies that are Hannan consistent \cite{Hannan1957_HannanConsistency_Hannan}, i.e, a policy $\pi\in \Pi^\psi$ such that $R_T^\psi (\pi)/T \rightarrow 0$.

To fix ideas let us consider SA-Problem with $2$ sensors. We enumerate all possible $8$ tuples $(Y, \hat{Y}^1, \hat{Y}^2)$ as shown in Table \ref{tab:SensorOutcomes}, and write probability of $i$th tuple $i=1,2,\cdots 8$ as $p_{i-1}$.  From Table \ref{tab:SensorOutcomes}, we have  $\gamma_1=p_2+p_3+p_4+p_5$ and $\gamma_2=p_1+p_3+p_4+p_6$, thus
\begin{equation}
	\gamma_1-\gamma_2 = p_2+p_5-p_1-p_6.
\end{equation}

\begin{minipage}{0.5\textwidth}
	\vspace{.5cm}
	\begin{tabular}[c]{ c|c|c|c } 
		\label{tab:SensorOutcomes}
	%			\caption{Possible binary tuples}
		$Y$ & $\hat{Y}^1$ & $\hat{Y}^2$ & $\Pr(Y, \hat{Y}^1, \hat{Y}^2)$ \\ \hline 
		$0$ & $0$ & $0$ & $p_0$ \\  \hline
		$0$ & $0$ & $1$ & $p_1$ \\  \hline
		$0$ & $1$ & $0$ & $p_2$ \\  \hline
		$0$ & $1$ & $1$ & $p_3$ \\  \hline
		$1$ & $0$ & $0$ & $p_4$ \\  \hline
		$1$ & $0$ & $1$ & $p_5$ \\  \hline
		$1$ & $1$ & $0$ & $p_6$ \\  \hline
		$1$ & $1$ & $1$ & $p_7$ \\  \hline
		
	\end{tabular}
		\vspace{.5cm}
\end{minipage}\hspace{-1.5cm}
\begin{minipage}[c]{0.6\textwidth}
		\vspace{.5cm}
		\centering
	\hspace{-5cm}		
\begin{equation}
\label{eqn:Marginals}
\Pr(\hat{Y}^1, \hat{Y}^2)=
\begin{cases}
p_1 + p_5 \mbox{  if } (\hat{Y}^1, \hat{Y}^2)=(0,1)\\
p_2 + p_6 \mbox{  if } (\hat{Y}^1, \hat{Y}^2)=(1,0)\\
p_0 + p_4 \mbox{  if } (\hat{Y}^1, \hat{Y}^2)=(0,0)\\
p_3 + p_7 \mbox{  if } (\hat{Y}^1, \hat{Y}^2)=(1,1)
\end{cases}
\end{equation}
	\vspace{.5cm}
\end{minipage}

\noindent
Since we only observe feedbacks $(\hat{Y}_t^1, \hat{Y}_t^2)$ and not the true labels $Y_t$, only marginal probabilities $\Pr(\hat{Y}^1, \hat{Y}^2)$ as given in (\ref{eqn:Marginals}) can be estimated but not $\Pr(Y, \hat{Y}^1, \hat{Y}^2)$. Thus all the decision has to be based on the marginals only. To see when SAP has a Hannan consistent policy, let us consider the following conditions.
\begin{condition}
	\label{cond:PathDominance1}
	 If sensor $1$ predicts label $1$ correctly, then sensor $2$ also predicts it correctly\footnote{Suppose we interpret label $1$ as 'threat', the condition implies that if sensor $1$ detects threat correctly, the better sensor $2$ also detects it. }, i.e.,
	 \begin{equation*}
	 \label{eqn:PathDominace1} 
	 Y_t=1 \mbox{ and } \hat{Y}_t^1=1 \implies \hat{Y}^2_t=1. 
	 \end{equation*}
\end{condition} 
\begin{condition}
		\label{cond:PathDominance2}
	If sensor $1$ predicts label $0$ correctly, then sensor $2$ also predicts it correctly, i.e.,
	\begin{equation*}
	\label{eqn:PathDominace2} 
	Y_t=0 \mbox{ and } \hat{Y}_t^1=0 \implies \hat{Y}^2_t=0. 
	\end{equation*}
\end{condition}
The following example demonstrate  marginals do not unambiguously decide optimal action under Condition \ref{cond:PathDominance1}.
Set $c=0.35$ and consider the following two cases: 1) $p_2=1/2, p_1=1/4-1/40, p_5=1/4+1/40$ and 2) $p_2=1/2, p_1=1/4-3/40,p_5=1/4+3/40$. From Condition (\ref{cond:PathDominance1}) we have $p_6=0$. Also, set $p_0=p_4=p_3=p_7=0$ in both the cases. We get $\gamma_1-\gamma_2=0.3$ in the first case,  whereas $\gamma_1-\gamma_2=0.4$ in the second case. From \ref{eqn:OptimalAction}, optimal action is $1$ in the first case, whereas it is  $2$ in the second case. However, for both the cases the marginals $\Pr(\hat{Y}^1, \hat{Y}^2)$ are the same for all pairs $(\hat{Y}^1, \hat{Y}^2)$. Since we only  observe $\Pr(\hat{Y}^1, \hat{Y}^2)$, the two cases cannot be distinguished and linear regret is unavoidable. We can argue similarly that Condition (\ref{cond:PathDominance2}) is not sufficient for sub-linear regret. 

Next, consider that both Condition (\ref{cond:PathDominance1}) and (\ref{cond:PathDominance2}) hold, i.e.,  
\begin{condition}
	\label{cond:PathDominance}
	If sensor $1$ is correct , then sensor $2$ is also correct, i.e.,
	\begin{equation*}
	\label{eqn:Dominace} 
	 \hat{Y}_t^1=Y_t \implies \hat{Y}^2_t=Y_t. 
	\end{equation*}
\end{condition}
Then, $p_1=p_6=0$ and we get $\gamma_1-\gamma_2=p_2+p_5$. Since $p_2+p_5=\Pr(\hat{Y}^1 \neq \hat{Y}^2)$, it can be estimated from observations $(\hat{Y}_t^1,\hat{Y}_t^2)$, and the optimal action can be found unambiguously. Thus Condition \ref{cond:PathDominance} gives a sufficient for existence of an Hannan consistent policy. In the following we refer to Condition (\ref{cond:PathDominance}) as strong dominance property. For the case of $K>2$ sensors, its definition is as follows: 

\begin{definition}[Strong Dominance]
	A SA-Problem is said to satisfy strong dominance property if sensor $i$ predicts correctly, then all the sensors in the subsequent stages of the cascade also predict correctly, i.e.,
	\begin{equation}
	\label{eqn:DominanceCondition}
	\hat{Y}_t^i=Y_t \rightarrow \hat{Y}_t^j \quad \forall j>i\geq 1.
	\end{equation}
\end{definition}

We will now establish necessary and sufficient conditions for SAP learnability
For notional convenience rewrite 
$\gamma_1- \gamma_2= p_1+p_2+p_5+p_6- 2(p_1+p_6):=p_{12}-2\delta$,
where $p_{12}:=\Pr(Y^1\neq Y^2)$ is the probability that sensors disagree and $\delta:=\Pr(Y^2 \neq Y | Y^1=Y)$ is the conditional probability that sensor $2$ is incorrect given that sensor $1$ is correct. We can estimate $p_{12}$ from feedback $(\hat{Y}^1_t, \hat{Y}^2_t)$, but $\delta$ cannot be estimated.
\begin{thm}
For SA-Problem with $K=2$, an Hannan consistent policy exists if and only if $c \notin [p_{12}-2\delta, p_{12}]$.
\end{thm}
{\bf Proof:} Under dominance condition $\delta=0$, thus actions $1$ is optimal if $p_{12}<c$, otherwise action $2$ is optimal. Suppose dominance condition is violated, i.e., $\delta>0$, but decisions are made assuming dominance condition holds (i.e., using estimates of $p_{12}$ only), then the optimal action is correctly identified provided $\delta$ is such that $p_{12}-2\delta < c \Rightarrow p_{12} <c$ or $p_{12}-2\delta >c \Rightarrow p_{12}>c$. Now, notice that the latter implication is always true. So, whenever action $2$ is optimal, violation of dominance condition does not miss the optimal action. However, the first implication holds if and only if $c \notin [p_{12}-2\delta, p_{12}]$.

Clearly, when $\delta$ is small Hannan consistent policy exits for a large range of $c$. For the case of $K>2$ sensors, its definition is as follows: 

\begin{definition}[Weak Dominance]
	A SA-Problem is said to satisfy weak dominance property if $c_k \notin [p_{k-1,k}-2\delta_{k-1,k}, p_{k-1,k}]$ for all $1<k<K$, where $p_{k-1,k}=\Pr(Y^{k-1}\neq Y^k)$ and $\delta_{k-1,k}=\Pr(Y^k\neq Y| Y^{k-1}=Y).$
\end{definition}

Many real world applications are designed to satisfy strong dominance property. For example, in wireless communication, increasing block length (more redundancy) improves tolerance against noise. Many practical datasets like, PIMA diabetes dataset and breast cancer dataset, conditional error probabilities are small.   (i will add numerical values)

In the following we establish that if dominance property holds efficient algorithms for a SAP problem can be derived from algorithms on a suitable stochastic multi-armed bandit problem. We first recall the stochastic multi-armed bandit setting and the relevant results. 

%\section{Problem Setup}
%\label{sec:Setup}
%We consider the problem of efficient label prediction under partial monitoring. Let $\{Y_t\}_{{t>0}}$ denote sequence of labels generated according to an unknown distribution $\mathcal{D}$.  The learner can use a `cheap' sensor (device-1) or/and a `costly' sensor (device-2) to predict the labels. In round $t$, let $\hat{Y}^1_t$ and $\hat{Y}^2_t$ denote the predictions of device-1 and device-2 respectively. We assume that device-1 has lower performance than device-2 in the sense that prediction error rate of device-1, denoted as 
%$\gamma_1:=\Pr\{Y_t\neq \hat{Y}^1_t\}$, is larger than or equal to that of device-2, denoted as $\gamma_2:=\Pr\{Y_t\neq \hat{Y}^2_t\}$ ($\gamma_1>\gamma_2$). In each round $t$, the learner can take the following actions: 
%\begin{itemize}
%	\item Action-1: use device-1.
% \item Action-2: use both the devices. 
%\end{itemize}
%For ease of notion, we denote actions by their index, and write $\mathcal{A}=\{1,2\}$ for the set of actions and use $i$ to index them. Let $F_t(i)$ denote the feedback observed in round $t$ by selecting action $i$. When $i=1$, the learner observes $\hat{Y}^1_t$, and when $i=2$, he observes both $\hat{Y}^1_t$ and $\hat{Y}^2_t$. That is, 
%\begin{equation}
%F_t(i)=\begin{cases}
%\hat{Y}_t^1 \;\;\mbox{if}\;\;i=1,\\
%\{\hat{Y}^1_t, \hat{Y}^2_t\} \;\;\mbox{if}\;\;i=2.
%\end{cases}
%\end{equation} 
%The loss incurred in each round is defined as follows. When $i=1$, the loss is $1$ unit if prediction of device-1 (observed feedback) is incorrect, otherwise loss is zero. When $i=2$, a fixed loss of $c>0$ is incurred in addition to the prediction loss of device-2, which is $1$ unit if device-2's prediction is incorrect and $0$ otherwise. Let $L_t(i)$ denote the loss in round $t$ for taking action $i\in \mathcal{A}$. Then,
%\begin{equation}
%L_t(i)=\begin{cases}
%\boldsymbol{1}_{\{\hat{Y}^1_t \neq Y_t\}} \;\;\mbox{if}\;\;i=1,\\
%\boldsymbol{1}_{\{\hat{Y}^2_t\neq Y_t\}}+c \;\;\mbox{if}\;\;i=2.
%\end{cases}
%\end{equation} 
%Let $H_{t}$ denote the set of actions played and the corresponding feedback observed till time $t$.  A policy $\pi=(\pi_1, \pi_2, \cdots)$, where  $\pi_t : H_{t-1}\rightarrow
%\mathcal{A}$, gives action selected in each round using all the feedback observed in the past. The expected regret of a policy $\pi$ that selects action $\pi_t \in\mathcal{A}$ in round $t$ over a period $T$ with respect to the best action in hindsight is given as 
%\begin{equation}
%\label{eqn:Regret2Actions}
%R_T(\pi)= \mathbb{E}\left [\sum_{t=1}^TL_t(\pi_t)\right]
%-\min_{i\in \mathcal{A}}\mathbb{E}\left [\sum_{t=1}^T L_t(i)\right ].
%\end{equation}
%The goal of the learner is to learn a policy that minimizes the maximum expected regret, i.e.,
%\begin{equation}
%\pi^*= \arg \min_{\pi \in \Pi }R_T(\pi)  ,
%\end{equation}
%where $\Pi$ denote the set of policies that maps past history to an action in $\mathcal{A}$ in each round. 
%
%\noindent
%{\bf Optimal action in hindsight: } For any $\pi \in \Pi$ and in any round $t$ we have 
%\begin{equation}
%\mathbb{E}[L_t(i)]=
%\begin{cases}
%\gamma_1\;\; \mbox{if}\;\; i=1,\\
%\gamma_2+c\;\; \mbox{if}\;\;i=2,
%\end{cases}
%\end{equation}
%Let  $i^*=\arg\min_{i \in \mathcal{A}} {E}[L_t(i)]$ denote optimal action. Then, $i^*=1$ if $\gamma_1 \leq \gamma_2 +c$, and $i^*=2$ otherwise. Let $I_t$ denote the action taken in round $t$ and $N_i(s)$ denote the number of times action $i$ is selected till time $s$, i.e., $N_i(s)=\sum_{t=1}^s \boldsymbol{1}_{\{I_t=i\}}$. The expected regret can be expressed as
%\begin{eqnarray}
%\label{eqn:ExpRegretGap}
%R_T(\pi)&=& \sum_{i=1}^{2}\mathbb{E}[N_i(T)]\Delta_i 
%\end{eqnarray}
%where $\Delta_1=\gamma_1 -\mathbb{E}[L(i^*)]$ and $
%\Delta_2=\gamma_2+c -\mathbb{E}[L(i^*)]$. Note that for all $i=1,2$, either $\Delta_i=|\gamma_1-\gamma_2-c|$ or $\Delta_i=0$.
%
%
%\noindent
%{\bf Assumptions (Dominance condition):}   Whenever device-1 makes no prediction error, device-2 is also guaranteed to make no prediction error, i.e., in every round $t$, 
%\begin{equation}
%\label{eqn:DomAssum}
%{\hat{Y}^1_t=Y_t} \implies {\hat{Y}^2_t=Y_t}.
%\end{equation}  
%\noindent
%{\bf Reduction to the apple tasting problem:}
%The feedback from action $i=1$ reveals no information about the loss incurred in that round. However feedback after action $i=2$ reveals (partial) information about the loss of both actions. Suppose feedback is such that the predictions of devices disagree, i.e., ${\hat{Y}^1_t\neq\hat{Y}^2_t}$ after action $2$.  The dominance condition then implies that the only possible events are $\hat{Y}^1_t \neq Y_t$ and $\hat{Y}^2_t=Y_t$. I.e., the true label is that predicted by device-2 and loss is zero. Suppose the predictions of the devices agree, i.e., ${\hat{Y}^1_t = \hat{Y}^2_t}$, then the dominance condition implies that either predictions of both are correct or both are incorrect. Though the true loss is not known in this case, the learner can infer some useful knowledge: in round $t$, if the prediction of both the devices agree, then the difference of loss of the actions is $L_t(2)-L_t(1)=c>0$. And if the predictions of the devices disagree, then dominance assumption implies that $L_t(1)=1$ and $L_t(2)=c$ or $L_t(2)-L_t(1)=c-1<0$. Thus, each time learner selects action $2$, he gets to know whether or not he was better off by selecting the other action. This setup is similar the standard apple tasting problem \cite{IC2000_AppleTasting_HelmboldLittlestoneLong}
%], but differs in terms of the information structure when action $2$ is played: in the apple tasting problem, playing action $2$ in a round reveals loss incurred by both the actions. Whereas, in the sensor selection problem we get only partial information on which of the two actions is better in that round. However, we will see below that the partial information is enough to distinguish the optimal arm and one can obtain performance similar to that in the apple tasting problem .

